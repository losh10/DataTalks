{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join('', 'data')\n",
    "train_path = os.path.join(base_path, 'Train.csv')\n",
    "test_path = os.path.join(base_path, 'Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['pm2_5'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape (8071, 80) and test_df shape: (2783, 79)\n"
     ]
    }
   ],
   "source": [
    "# shape of the train and test datasets\n",
    "print(f'train_df shape {train_df.shape} and test_df shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of columns to drop is 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# removing variables with more than 45% missing values\n",
    "drop_cols = [i for i in train_df.columns if train_df[i].isnull().sum() / len(train_df) > 0.45]\n",
    "print(f'the number of columns to drop is {len(np.array(drop_cols))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carbonmonoxide_co_column_number_density', 'carbonmonoxide_h2o_column_number_density', 'carbonmonoxide_cloud_height', 'carbonmonoxide_sensor_altitude', 'carbonmonoxide_sensor_azimuth_angle', 'carbonmonoxide_sensor_zenith_angle', 'carbonmonoxide_solar_azimuth_angle', 'carbonmonoxide_solar_zenith_angle', 'uvaerosolindex_absorbing_aerosol_index', 'uvaerosolindex_sensor_altitude', 'uvaerosolindex_sensor_azimuth_angle', 'uvaerosolindex_sensor_zenith_angle', 'uvaerosolindex_solar_azimuth_angle', 'uvaerosolindex_solar_zenith_angle', 'ozone_o3_column_number_density', 'ozone_o3_column_number_density_amf', 'ozone_o3_slant_column_number_density', 'ozone_o3_effective_temperature', 'ozone_cloud_fraction', 'ozone_sensor_azimuth_angle', 'ozone_sensor_zenith_angle', 'ozone_solar_azimuth_angle', 'ozone_solar_zenith_angle', 'cloud_cloud_fraction', 'cloud_cloud_top_pressure', 'cloud_cloud_top_height', 'cloud_cloud_base_pressure', 'cloud_cloud_base_height', 'cloud_cloud_optical_depth', 'cloud_surface_albedo', 'cloud_sensor_azimuth_angle', 'cloud_sensor_zenith_angle', 'cloud_solar_azimuth_angle', 'cloud_solar_zenith_angle']\n",
      "total number of columns with missing values is 34\n"
     ]
    }
   ],
   "source": [
    "null_cols = [i for i in train_df.columns if train_df[i].isnull().sum() > 1]\n",
    "print(null_cols, end='\\n')\n",
    "print(f'total number of columns with missing values is {len(np.array(null_cols))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64')]\n",
      "there are 34 missing values with data typefloat64\n"
     ]
    }
   ],
   "source": [
    "# find the dtype of the missing columns\n",
    "d_types = []\n",
    "for dt in null_cols:\n",
    "    d_types.append(train_df[dt].dtypes)\n",
    "\n",
    "print(d_types, end='\\n')\n",
    "print(f'there are {d_types.count(d_types[0])} missing values with data type{d_types[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of the missing values are continous in nature therefore we can use imputation methods like:...(to explore later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_variables(df, numeric_variables):\n",
    "    num_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_variables] = num_imputer.fit_transform(df[numeric_variables])\n",
    "    return df\n",
    "train_df_imputed = impute_variables(df=train_df, numeric_variables=null_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8071 entries, 0 to 8070\n",
      "Data columns (total 44 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   id                                        8071 non-null   object \n",
      " 1   site_id                                   8071 non-null   object \n",
      " 2   site_latitude                             8071 non-null   float64\n",
      " 3   site_longitude                            8071 non-null   float64\n",
      " 4   city                                      8071 non-null   object \n",
      " 5   country                                   8071 non-null   object \n",
      " 6   date                                      8071 non-null   object \n",
      " 7   hour                                      8071 non-null   int64  \n",
      " 8   month                                     8071 non-null   float64\n",
      " 9   carbonmonoxide_co_column_number_density   8071 non-null   float64\n",
      " 10  carbonmonoxide_h2o_column_number_density  8071 non-null   float64\n",
      " 11  carbonmonoxide_cloud_height               8071 non-null   float64\n",
      " 12  carbonmonoxide_sensor_altitude            8071 non-null   float64\n",
      " 13  carbonmonoxide_sensor_azimuth_angle       8071 non-null   float64\n",
      " 14  carbonmonoxide_sensor_zenith_angle        8071 non-null   float64\n",
      " 15  carbonmonoxide_solar_azimuth_angle        8071 non-null   float64\n",
      " 16  carbonmonoxide_solar_zenith_angle         8071 non-null   float64\n",
      " 17  uvaerosolindex_absorbing_aerosol_index    8071 non-null   float64\n",
      " 18  uvaerosolindex_sensor_altitude            8071 non-null   float64\n",
      " 19  uvaerosolindex_sensor_azimuth_angle       8071 non-null   float64\n",
      " 20  uvaerosolindex_sensor_zenith_angle        8071 non-null   float64\n",
      " 21  uvaerosolindex_solar_azimuth_angle        8071 non-null   float64\n",
      " 22  uvaerosolindex_solar_zenith_angle         8071 non-null   float64\n",
      " 23  ozone_o3_column_number_density            8071 non-null   float64\n",
      " 24  ozone_o3_column_number_density_amf        8071 non-null   float64\n",
      " 25  ozone_o3_slant_column_number_density      8071 non-null   float64\n",
      " 26  ozone_o3_effective_temperature            8071 non-null   float64\n",
      " 27  ozone_cloud_fraction                      8071 non-null   float64\n",
      " 28  ozone_sensor_azimuth_angle                8071 non-null   float64\n",
      " 29  ozone_sensor_zenith_angle                 8071 non-null   float64\n",
      " 30  ozone_solar_azimuth_angle                 8071 non-null   float64\n",
      " 31  ozone_solar_zenith_angle                  8071 non-null   float64\n",
      " 32  cloud_cloud_fraction                      8071 non-null   float64\n",
      " 33  cloud_cloud_top_pressure                  8071 non-null   float64\n",
      " 34  cloud_cloud_top_height                    8071 non-null   float64\n",
      " 35  cloud_cloud_base_pressure                 8071 non-null   float64\n",
      " 36  cloud_cloud_base_height                   8071 non-null   float64\n",
      " 37  cloud_cloud_optical_depth                 8071 non-null   float64\n",
      " 38  cloud_surface_albedo                      8071 non-null   float64\n",
      " 39  cloud_sensor_azimuth_angle                8071 non-null   float64\n",
      " 40  cloud_sensor_zenith_angle                 8071 non-null   float64\n",
      " 41  cloud_solar_azimuth_angle                 8071 non-null   float64\n",
      " 42  cloud_solar_zenith_angle                  8071 non-null   float64\n",
      " 43  pm2_5                                     8071 non-null   float64\n",
      "dtypes: float64(38), int64(1), object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data into train, validation and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_full, df_test = train_test_split(train_df_imputed, test_size=0.2, random_state=17)\n",
    "\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=17)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train['pm2_5']\n",
    "y_val = df_val['pm2_5']\n",
    "y_test = df_test['pm2_5']\n",
    "\n",
    "del df_train['pm2_5']\n",
    "del df_val['pm2_5']\n",
    "del df_test['pm2_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of the train, validation and test set are: ((4842, 43), (1614, 43), (1615, 43))\n"
     ]
    }
   ],
   "source": [
    "print(f'shapes of the train, validation and test set are: {df_train.shape, df_val.shape, df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a Dictvectorizer to the  data\n",
    "\n",
    "dv = DictVectorizer(sparse=True)\n",
    "x_train_dv= dv.fit_transform(df_train.to_dict(orient='records'))\n",
    "x_val_dv= dv.transform(df_val.to_dict(orient='records'))\n",
    "x_test_dv= dv.transform(df_test.to_dict(orient='records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Baseline model \n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.25\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=10, random_state=17, n_jobs=-1)\n",
    "model.fit(x_train_dv, y_train)\n",
    "\n",
    "#rmse\n",
    "y_val_preds = model.predict(x_val_dv)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_preds))\n",
    "print(f\"{rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our baseline model\n",
    "# The goal is to make thi model perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
